import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from model import GPT2Transformer
from CreateTokernizerAndData.tokenizer_custom import build_vocab_from_file, encode, decode

import os

# =====================
# é…ç½®å‚æ•°
# =====================
train_file = 'train.txt'
batch_size = 16
max_len = 128
num_epochs = 10
learning_rate = 5e-4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_path = "gpt2_qa_model.pth"  # ä¿å­˜æ¨¡å‹çš„è·¯å¾„

# =====================
# è¯»å–è¯è¡¨
# =====================
token2id, id2token = build_vocab_from_file(train_file)
vocab_size = len(token2id)
print("ğŸš€è¯è¡¨å¤§å°ï¼š",vocab_size)

# =====================
# è‡ªå®šä¹‰æ•°æ®é›†
# =====================
class QADataset(Dataset):
    def __init__(self, filepath, token2id, max_len):
        self.samples = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                ids = encode(line, token2id, max_len)
                self.samples.append(torch.tensor(ids))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        x = self.samples[idx][:-1]  # è¾“å…¥
        y = self.samples[idx][1:]   # ç›®æ ‡ï¼ˆä¸‹ä¸€ä¸ª tokenï¼‰
        return x, y

# =====================
# æ•°æ®åŠ è½½
# =====================
dataset = QADataset(train_file, token2id, max_len)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# =====================
# æ„å»ºæ¨¡å‹
# =====================
model = GPT2Transformer(
    vocab_size=vocab_size,
    d_model=512,
    num_heads=8,
    num_layers=6,
    max_len=max_len,
    dropout=0.1
).to(device)

total_params = sum(p.numel() for p in model.parameters())
print(f"ğŸš€ å½“å‰æ¨¡å‹çš„æ€»å‚æ•°é‡: {total_params:,}")
criterion = nn.CrossEntropyLoss(ignore_index=token2id['<pad>'])
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# =====================
# åŠ è½½æ¨¡å‹å‚æ•°çš„å‡½æ•°
# =====================
def load_model(model, model_path):
    if os.path.exists(model_path):
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹å‚æ•°: {model_path}")
        model.load_state_dict(torch.load(model_path))
    else:
        print("âš ï¸ æ¨¡å‹å‚æ•°æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ— æ³•åŠ è½½!")

# =====================
# è®­ç»ƒå‡½æ•°
# =====================
def train(model, data_loader, criterion, optimizer, num_epochs, model_path=None):
    # ä»…åŠ è½½æ¨¡å‹å‚æ•°
    if model_path:
        load_model(model, model_path)
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒï¼")
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0.0
        for batch_idx, (x, y) in enumerate(data_loader):
            x = x.to(device)
            y = y.to(device)
            output = model(x)  # [batch, seq_len, vocab_size]
            loss = criterion(output.view(-1, vocab_size), y.view(-1))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(data_loader)
        print(f"ğŸ“… Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}")

        # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ¨¡å‹
        torch.save(model.state_dict(), model_path)
        print(f"ğŸ’¾ Epoch {epoch+1} æ¨¡å‹å·²ä¿å­˜")

# =====================
# å¼€å§‹è®­ç»ƒæˆ–åŠ è½½å¹¶ç»§ç»­è®­ç»ƒ
# =====================
train(model, data_loader, criterion, optimizer, num_epochs, model_path=model_path)

# =====================
# ä¿å­˜æœ€ç»ˆæ¨¡å‹
# =====================
torch.save(model.state_dict(), model_path)
print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ä¸º {model_path}")
