import torch
import json

from model import GPT2Transformer
from CreateTokernizerAndData.tokenizer_custom import build_vocab_from_file, decode, encode_question

# =====================
# å‚æ•°é…ç½®
# =====================
train_file = 'train.txt'
model_path = 'saved_models/final_model.pth'
max_len = 128
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =====================
# åŠ è½½è¯è¡¨
# =====================
token2id, id2token = build_vocab_from_file(train_file)
vocab_size = len(token2id)

# =====================
# åŠ è½½æ¨¡å‹
# =====================
model = GPT2Transformer(
    vocab_size=vocab_size,
    d_model=512,
    num_heads=8,
    num_layers=6,
    max_len=max_len,
    dropout=0.1
).to(device)

# model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()
print("âœ… æ¨¡å‹å·²æˆåŠŸåŠ è½½å¹¶è¿›å…¥æ¨ç†æ¨¡å¼ï¼")
print("è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰ï¼š")

# =====================
# æ¨ç†å‡½æ•°
# =====================
while True:
    question = input("ğŸ“ é—®é¢˜: ")
    if question.strip().lower() in ['q', 'quit', 'exit']:
        print("ğŸ‘‹ æ¨ç†ç»“æŸï¼Œæ¬¢è¿å†æ¬¡ä½¿ç”¨ï¼")
        break

    # ç¼–ç è¾“å…¥
    input_ids = encode_question(question, token2id, max_len)
    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)

    # æ¨¡å‹æ¨ç†
    with torch.no_grad():
        output = model(input_tensor)

    # å– logits ä¸­ <sep> åé¢çš„é¢„æµ‹ç»“æœ
    output_ids = torch.argmax(output, dim=-1)[0].tolist()

    # æŸ¥æ‰¾ <sep> çš„ä½ç½®ï¼Œè·³è¿‡é—®é¢˜éƒ¨åˆ†ï¼Œä»…æ˜¾ç¤ºå›ç­”å†…å®¹
    try:
        sep_index = input_ids.index(token2id["<sep>"])
        predicted_answer_ids = output_ids[sep_index + 1:]
    except ValueError:
        predicted_answer_ids = output_ids  # å¦‚æœæ²¡æœ‰<sep>ï¼Œå…¨éƒ¨å±•ç¤º

    # è§£ç è¾“å‡º
    answer = decode(predicted_answer_ids, id2token)
    answer = answer.split("<eos>")[0]  # æˆªæ–­è‡³ <eos>

    print("ğŸ¤– ç­”æ¡ˆ:", answer.strip())
