import torch
from model import GPT2Transformer
from CreateTokernizerAndData.tokenizer_custom import build_vocab_from_file, encode, decode
import os

# =====================
# é…ç½®å‚æ•°
# =====================
test_file = 'train.txt'  # æµ‹è¯•æ–‡ä»¶è·¯å¾„
batch_size = 1  # æµ‹è¯•æ—¶æ‰¹å¤§å°é€šå¸¸è®¾ä¸º 1
max_len = 128
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_path = "gpt2_qa_model.pth"  # å·²è®­ç»ƒæ¨¡å‹è·¯å¾„

# =====================
# è¯»å–è¯è¡¨
# =====================
token2id, id2token = build_vocab_from_file(test_file)
vocab_size = len(token2id)
print("ğŸš€è¯è¡¨å¤§å°ï¼š", vocab_size)

# =====================
# æµ‹è¯•æ•°æ®é›†
# =====================
class QADataset(torch.utils.data.Dataset):
    def __init__(self, filepath, token2id, max_len):
        self.samples = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                ids = encode(line, token2id, max_len)
                self.samples.append(torch.tensor(ids))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        x = self.samples[idx]  # ç›´æ¥ä½¿ç”¨æ•´ä¸ªæ ·æœ¬ä½œä¸ºè¾“å…¥
        return x

# =====================
# åŠ è½½æ¨¡å‹
# =====================
def load_model(model, model_path):
    if os.path.exists(model_path):
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹å‚æ•°: {model_path}")
        model.load_state_dict(torch.load(model_path))
        model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    else:
        print("âš ï¸ æ¨¡å‹å‚æ•°æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ— æ³•åŠ è½½!")

# =====================
# æµ‹è¯•å‡½æ•°
# =====================
def test(model, data_loader, device, id2token):
    print("ğŸš€ å¼€å§‹æµ‹è¯•ï¼")
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    with torch.no_grad():
        for batch_idx, x in enumerate(data_loader):
            x = x.to(device)

            output = model(x)
            output = output.argmax(dim=-1)  # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token

            decoded_output = decode(output[0], id2token)
            print(f"Input: {decode(x[0], id2token)}")
            print(f"Predicted Output: {decoded_output}")

# =====================
# æ•°æ®åŠ è½½
# =====================
dataset = QADataset(test_file, token2id, max_len)
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)

# =====================
# æ„å»ºæ¨¡å‹
# =====================
model = GPT2Transformer(
    vocab_size=vocab_size,
    d_model=512,
    num_heads=8,
    num_layers=6,
    max_len=max_len,
    dropout=0.1
).to(device)

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
load_model(model, model_path)

# =====================
# å¼€å§‹æµ‹è¯•
# =====================
test(model, data_loader, device, id2token)
