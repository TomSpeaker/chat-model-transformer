import json
from tokenizer_custom import load_vocab_from_file, encode

# ========= é…ç½®é¡¹ =========
max_len = 128
input_file = "train.txt"
output_file = "train_encoded_v2.jsonl"
vocab_file = "vocab.json"

# ========= ç‰¹æ®Š Token =========
BOS_TOKEN = "<bos>"
SEP_TOKEN = "<sep>"
EOS_TOKEN = "<eos>"
PAD_TOKEN = "<pad>"

# ========= åŠ è½½è¯è¡¨ =========
token2id, id2token = load_vocab_from_file(vocab_file)
print(f"âœ… è¯è¡¨è¯»å–æˆåŠŸï¼Œè¯è¡¨å¤§å°: {len(token2id)}")

# æ·»åŠ ç‰¹æ®Š tokenï¼ˆå¦‚æœç¼ºå¤±ï¼‰
special_tokens = [BOS_TOKEN, SEP_TOKEN, EOS_TOKEN, PAD_TOKEN]
for token in special_tokens:
    if token not in token2id:
        token2id[token] = len(token2id)
        id2token[len(id2token)] = token

# è·å–ç‰¹æ®Š token çš„ ID
bos_token_id = token2id[BOS_TOKEN]
sep_token_id = token2id[SEP_TOKEN]
eos_token_id = token2id[EOS_TOKEN]
pad_token_id = token2id[PAD_TOKEN]

# ========= æ ¸å¿ƒå‡½æ•°ï¼šå¤„ç†å•è¡Œ =========
def process_line(line):
    line = line.strip()

    if not line.startswith(BOS_TOKEN) or SEP_TOKEN not in line or EOS_TOKEN not in line:
        return None  # æ ¼å¼ä¸åˆæ³•

    try:
        # æ‹†åˆ†ä¸º question å’Œ answer
        question_part, answer_part = line.split(SEP_TOKEN, 1)
        question = question_part.strip()
        answer = answer_part.strip()

        # è‡ªåŠ¨çº æ­£ç‰¹æ®Š tokenï¼ˆé˜²æ­¢ç¼ºå¤±ï¼‰
        if not question.startswith(BOS_TOKEN):
            question = BOS_TOKEN + question
        if not question.endswith(EOS_TOKEN):
            question += EOS_TOKEN
        if not answer.startswith(BOS_TOKEN):
            answer = BOS_TOKEN + answer
        if not answer.endswith(EOS_TOKEN):
            answer += EOS_TOKEN

        # ç¼–ç 
        input_ids = encode(question, token2id)
        labels = encode(answer, token2id)

        # å¦‚æœ input_ids æˆ– labels ä¸­æ²¡æœ‰ <eos>ï¼Œåˆ™ä¸¢å¼ƒè¯¥æ¡æ•°æ®
        if eos_token_id not in input_ids or eos_token_id not in labels:
            return None

        # å¡«å……åˆ° max_len
        input_ids += [pad_token_id] * (max_len - len(input_ids))
        labels += [pad_token_id] * (max_len - len(labels))

        return {"input_ids": input_ids, "labels": labels}

    except Exception as e:
        print(f"âŒ å¤„ç†å‡ºé”™: {e}")
        return None

# ========= æ‰§è¡Œå¤„ç† =========
sample_count = 0
with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8') as fout:
    for line in fin:
        sample = process_line(line)
        if sample:
            fout.write(json.dumps(sample, ensure_ascii=False) + "\n")
            sample_count += 1

print(f"ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {sample_count} æ¡è®­ç»ƒæ ·æœ¬ï¼Œè¾“å‡ºæ–‡ä»¶ï¼š{output_file}")
