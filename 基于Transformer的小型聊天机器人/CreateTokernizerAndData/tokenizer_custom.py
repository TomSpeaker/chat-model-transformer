import json
from collections import Counter
import os

# =============================
# Step 1: æ„å»ºè¯è¡¨
# =============================
def build_vocab_from_file(filename, min_freq=1):
    counter = Counter()

    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            for ch in line:
                counter[ch] += 1

    # ç‰¹æ®Šç¬¦å·
    special_tokens = ['<pad>', '<unk>', '<bos>', '<sep>', '<eos>']
    vocab = special_tokens + [token for token, freq in counter.items() if freq >= min_freq]

    token2id = {token: idx for idx, token in enumerate(vocab)}
    id2token = {idx: token for token, idx in token2id.items()}

    print(f"ğŸ“˜ è¯è¡¨å¤§å°: {len(vocab)}")
    return token2id, id2token

# =============================
# Step 2: ç¼–ç å‡½æ•°
# =============================
def encode(text, token2id, max_len=128):
    tokens = []
    i = 0
    special_tokens = ['<bos>', '<sep>', '<eos>']
    while i < len(text):
        matched = False
        for token in special_tokens:
            if text[i:i+len(token)] == token:
                tokens.append(token)
                i += len(token)
                matched = True
                break
        if not matched:
            tokens.append(text[i])
            i += 1

    ids = [token2id.get(tok, token2id['<unk>']) for tok in tokens]
    ids = ids[:max_len]
    ids += [token2id['<pad>']] * (max_len - len(ids))
    return ids


# =============================
# Step 3: è§£ç å‡½æ•°
# =============================
def decode(ids, id2token):
    tokens = [id2token.get(i, '<unk>') for i in ids]
    # å»é™¤ <pad>ï¼Œå¹¶ç¡®ä¿æ ¼å¼æ­£ç¡®
    text = ''.join([tok for tok in tokens if tok != '<pad>'])
    return text

# filename = 'train.txt'

# # Step 1: æ„å»ºè¯è¡¨
# token2id, id2token = build_vocab_from_file(filename)


# # Step 2: è¯»å–train.txtå¹¶ç¼–ç å‰åä¸ªå¥å­
# with open(filename, 'r', encoding='utf-8') as f:
#     lines = f.readlines()

# Step 3: ç¼–ç å¹¶è§£ç å‰åä¸ªå¥å­
# for i in range(min(1, len(lines))):  # è·å–å‰åè¡Œæˆ–æ–‡ä»¶ä¸­çš„æ‰€æœ‰è¡Œ
#     line = lines[i].strip()

#     # ç¼–ç 
#     encoded = encode(line, token2id)
#     # è§£ç 
#     decoded = decode(encoded, id2token)

#     # è¾“å‡ºåŸæ–‡ã€ç¼–ç åçš„å‰å‡ ä¸ªtokenå’Œè§£ç åçš„ç»“æœ
#     print(f"åŸæ–‡: {line}")
#     print(f"ç¼–ç åçš„å‰å‡ ä¸ªç»“æœ: {encoded[:10]}...")  # åªæ˜¾ç¤ºç¼–ç åçš„å‰10ä¸ª
#     print(f"è§£ç åçš„ç»“æœ: {decoded}")
#     print("=" * 50)

def encode_question(question: str, token2id: dict, max_len: int = 128) -> list:
    """
    ç»™å®šé—®é¢˜å†…å®¹ï¼Œç”ŸæˆåŠ ä¸Š <bos> å’Œ <sep> çš„ç¼–ç åºåˆ—ã€‚
    """
    special_tokens = ["<bos>", "<sep>"]
    input_text = special_tokens[0] + question + special_tokens[1]

    # æŠŠæ¯ä¸ªå­—ç¬¦ä½œä¸º token è¿›è¡Œç¼–ç 
    input_ids = [token2id.get(char, token2id.get("<unk>", 1)) for char in input_text]

    # è¡¥é½åˆ° max_len
    if len(input_ids) < max_len:
        input_ids += [token2id.get("<pad>", 0)] * (max_len - len(input_ids))
    else:
        input_ids = input_ids[:max_len]

    return input_ids
