import json
from collections import Counter

# =============================
# Step 1: æ„å»ºè¯è¡¨
# =============================
def build_vocab_from_file(filename, min_freq=1):
    counter = Counter()

    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            for ch in line:
                counter[ch] += 1

    # ç‰¹æ®Šç¬¦å·
    special_tokens = ['<pad>', '<unk>', '<bos>', '<sep>', '<eos>']
    vocab = special_tokens + [token for token, freq in counter.items() if freq >= min_freq]

    token2id = {token: idx for idx, token in enumerate(vocab)}
    id2token = {idx: token for token, idx in token2id.items()}

    print(f"ğŸ“˜ è¯è¡¨å¤§å°: {len(vocab)}")
    return token2id, id2token

# =============================
# Step 2: ä¿å­˜è¯è¡¨åˆ°æ–‡ä»¶
# =============================
def save_vocab_to_file(token2id, id2token, filepath):
    vocab_data = {
        'token2id': token2id,
        'id2token': {str(k): v for k, v in id2token.items()}  # key è½¬å­—ç¬¦ä¸²ï¼Œé¿å… JSON é—®é¢˜
    }
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(vocab_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… è¯è¡¨å·²ä¿å­˜åˆ° {filepath}")

# =============================
# Step 3: ä»æ–‡ä»¶åŠ è½½è¯è¡¨
# =============================
def load_vocab_from_file(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        vocab_data = json.load(f)
    token2id = vocab_data['token2id']
    id2token = {int(k): v for k, v in vocab_data['id2token'].items()}  # key è½¬å›æ•´æ•°
    print(f"ğŸ“‚ è¯è¡¨å·²ä» {filepath} åŠ è½½")
    return token2id, id2token

# =============================
# Step 4: ç¼–ç å‡½æ•°
# =============================
def encode(text, token2id, max_len=128):
    # æ·»åŠ  <bos> å’Œ <eos>
    tokens = ['<bos>'] + list(text) + ['<eos>']

    # è½¬æˆ ID
    ids = [token2id.get(tok, token2id['<unk>']) for tok in tokens]

    # è¡¥é½æˆ–æˆªæ–­
    ids = ids[:max_len]
    ids += [token2id['<pad>']] * (max_len - len(ids))

    return ids

# =============================
# Step 5: è§£ç å‡½æ•°
# =============================
def decode(ids, id2token):
    tokens = [id2token.get(i, '<unk>') for i in ids]

    # è¿‡æ»¤æ‰ç‰¹æ®Šç¬¦å·
    text = ''.join([tok for tok in tokens if tok not in ['<pad>', '<unk>', '<bos>', '<eos>']])
    return text

# =============================
# ç¤ºä¾‹ä½¿ç”¨
# =============================
if __name__ == '__main__':
    train_file = 'train.txt'      # æ„å»ºè¯è¡¨çš„æ•°æ®
    vocab_file = 'vocab.json'     # è¯è¡¨ä¿å­˜ä½ç½®

    # 1. æ„å»ºè¯è¡¨
    token2id, id2token = build_vocab_from_file(train_file)

    # 2. ä¿å­˜è¯è¡¨
    save_vocab_to_file(token2id, id2token, vocab_file)

    # 3. åŠ è½½è¯è¡¨
    loaded_token2id, loaded_id2token = load_vocab_from_file(vocab_file)

    # 4. æµ‹è¯•ç¼–ç  & è§£ç 
    question = "ä½ å¥½"
    encoded = encode(question, loaded_token2id, max_len=40)
    decoded = decode(encoded, loaded_id2token)

    print(f"\nğŸŸ¢ åŸå§‹é—®é¢˜: {question}")
    print(f"ğŸ”¢ ç¼–ç åçš„å‰å‡ ä¸ª token: {encoded[:10]}...")
    print(f"ğŸ”¤ è§£ç åçš„é—®é¢˜: {decoded}")
