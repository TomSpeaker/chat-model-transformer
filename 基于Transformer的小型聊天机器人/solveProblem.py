from CreateTokernizerAndData.tokenizer_custom import load_vocab_from_file, encode,decode
import torch
from model import GPT2Transformer

# ===================== å‚æ•°é…ç½® =====================
vocab_file = 'vocab.json'  # ç”¨äºæ„å»ºè¯è¡¨çš„æ–‡ä»¶
model_path = 'saved_models/final_model.pth'
max_len = 128
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ===================== åŠ è½½è¯è¡¨ =====================
token2id, id2token = load_vocab_from_file(vocab_file)
vocab_size = len(token2id)

# ===================== åŠ è½½æ¨¡å‹ =====================
model = GPT2Transformer(
    vocab_size=vocab_size,
    d_model=512,
    num_heads=8,
    num_layers=6,
    max_len=max_len,
    dropout=0.1
).to(device)

model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()
print("âœ… æ¨¡å‹å·²æˆåŠŸåŠ è½½å¹¶è¿›å…¥æ¨ç†æ¨¡å¼ï¼")

# ===================== ç¤ºä¾‹æ¨ç† =====================
# ç¤ºä¾‹è¾“å…¥
# 
while True:
    question = input("è¯·è¾“å…¥é—®é¢˜:")
    question = '<bos>'+question+'<eos>'
    # ç¼–ç è¾“å…¥
    input_ids = encode(question, token2id, max_len=max_len)  # List[int]
    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]

    # è§£ç åŸå§‹è¾“å…¥
    decoded_input = decode(input_ids, id2token)

    # æ¨¡å‹æ¨ç†
    with torch.no_grad():
        outputs = model(input_tensor)  # è¾“å‡ºå½¢çŠ¶: [1, seq_len, vocab_size]
        predictions = torch.argmax(outputs, dim=-1)  # [1, seq_len]
        predicted_ids = predictions[0].tolist()  # List[int]

    # è§£ç æ¨¡å‹è¾“å‡º
    decoded_output = decode(predicted_ids, id2token)
    # ===================== æ‰“å°ç»“æœ =====================
    print(f"ğŸŸ¡ åŸå§‹é—®é¢˜: {question}")
    print(f"ğŸŸ  æ¨¡å‹è¾“å‡ºæ–‡æœ¬: {decoded_output}")
    