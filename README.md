# ğŸ“Œ Introductionå‰è¨€

åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„ä¸­æ–‡ GPT-2 æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œèƒ½å¤Ÿè¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç³»ç»Ÿä»¥åŠå¯¹è¯ç³»ç»Ÿç­‰ã€‚æœ¬é¡¹ç›®é€šè¿‡æ„å»ºä¸€ä¸ªå®Œæ•´çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæä¾›äº†ç”¨äºè®­ç»ƒã€è°ƒä¼˜å’Œæ¨ç†çš„å®ç°ï¼Œå¹¶å…è®¸ç”¨æˆ·æ ¹æ®éœ€è¦è°ƒæ•´è¶…å‚æ•°å’Œæ¨¡å‹ç»“æ„ã€‚

é€šè¿‡æœ¬é¡¹ç›®ï¼Œä½ å°†èƒ½å¤Ÿæ·±å…¥äº†è§£ Transformer æ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œå¹¶ä¸”å­¦ä¼šå¦‚ä½•ç”¨ PyTorch æ¥å®ç°ä¸€ä¸ªå…·å¤‡ä¸­æ–‡å¤„ç†èƒ½åŠ›çš„ GPT-2 æ¨¡å‹ã€‚

- æ­¤å¼€æºé¡¹ç›®æ—¨åœ¨å®Œå…¨ä»0å¼€å§‹ï¼Œä»…ç”¨ä¸åˆ°ä¸€ä¸ª1hï¼å³å¯è®­ç»ƒå‡ºä»…ä¸º100Må‚æ•°çš„ä¼ ç»ŸTransformeræ¨¡å‹ã€‚
- ä½¿ç”¨çš„æ˜¯é˜¿é‡Œäº‘å¤©æ± å®éªŒå®¤çš„å•å¼ A10æ˜¾å¡

In this project, we will build a Chinese GPT-2 model based on the Transformer architecture. This model leverages deep learning techniques to perform natural language processing tasks, including text generation, question answering systems, and conversational systems. By constructing a complete neural network model, this project provides implementations for training, fine-tuning, and inference, while also allowing users to adjust hyperparameters and model structures as needed.

Through this project, you will gain a deep understanding of how Transformer models work and learn how to implement a Chinese GPT-2 model using PyTorch.

- This open-source project aims to start completely from scratch, and it only takes less than 1 hour to train a traditional Transformer model with only 100M parameters.
- The training is conducted using a single A10 GPU from Alibaba Cloud's Tianchi Lab.

>  æœ¬é¡¹ç›®ä¸­çš„ Transformer æ¨¡å‹å‚è€ƒè‡ª Vaswani et al. æå‡ºçš„ã€ŠAttention is All You Needã€‹è®ºæ–‡ï¼ˆ2017ï¼‰ã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹æ¶æ„ï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸã€‚

>  è®ºæ–‡é“¾æ¥ï¼š[Attention is All You Need](https://arxiv.org/abs/1706.03762)

>  æ„Ÿè°¢åŸä½œè€…çš„è´¡çŒ®ï¼

***

> In this project, the Transformer model is inspired by the paper "Attention is All You Need" by Vaswani et al. (2017). This paper introduced a model architecture based on self-attention mechanisms, which has revolutionized the field of Natural Language Processing (NLP).

> Paper link: [Attention is All You Need](https://arxiv.org/abs/1706.03762)

> We extend our gratitude to the original authors for their contributions!

# ğŸ“ŒProject Introductioné¡¹ç›®ä»‹ç»

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªåŸºäº Transformer çš„ GPT-2 æ¨¡å‹ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š

- **è¯è¡¨æ„å»º**ï¼šæ ¹æ®è¾“å…¥æ–‡æœ¬æ–‡ä»¶è‡ªåŠ¨æ„å»ºè¯è¡¨ï¼Œæ”¯æŒè‡ªå®šä¹‰æœ€å°è¯é¢‘ç­›é€‰ã€‚
- **æ¨¡å‹è®­ç»ƒ**ï¼šå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒæ–‡æœ¬ç¼–ç å’Œè§£ç åŠŸèƒ½ã€‚
- **æ¨ç†ä¸ç”Ÿæˆ**ï¼šå¯ä»¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œé€‚ç”¨äºå¯¹è¯ç”Ÿæˆå’Œé—®ç­”ä»»åŠ¡ã€‚
- **æ¨¡å‹ç»“æ„**ï¼šé‡‡ç”¨äº†æ ‡å‡†çš„ GPT-2 æ¶æ„ï¼ŒåŒ…æ‹¬ä½ç½®ç¼–ç ã€å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œç­‰ç»„ä»¶ã€‚

é¡¹ç›®çš„æ ¸å¿ƒå®ç°ä½¿ç”¨ PyTorch æ¡†æ¶ï¼Œé€šè¿‡å¯¹å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®çš„è®­ç»ƒæ¥è¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼Œå¹¶ä¸”å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸­æ–‡æ–‡æœ¬ï¼Œå¹¶æ”¯æŒå¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚

ğŸš€ å½“å‰æ¨¡å‹çš„æ€»å‚æ•°é‡: 24,915,679

This project implements a Transformer-based GPT-2 model, which includes the following features:

- **Vocabulary Construction**: Automatically constructs a vocabulary based on the input text file, supporting custom minimum frequency filtering.
- **Model Training**: Allows training the model using a custom dataset, with text encoding and decoding functionalities.
- **Inference and Generation**: Can load the trained model for text generation, suitable for dialogue generation and question-answering tasks.
- **Model Architecture**: Adopts the standard GPT-2 architecture, which includes components such as positional encoding, multi-head self-attention mechanism, and feed-forward networks.

The core implementation of the project uses the PyTorch framework. It trains a language model on large-scale text data and allows fine-tuning based on specific tasks. The model is capable of processing Chinese text and supports tasks such as dialogue generation.

ğŸš€ Current model's total parameter count: 24,915,679

# ğŸ“ŒProject Showé¡¹ç›®å±•ç¤º

**Generating Formatted Dataç”Ÿæˆæ ¼å¼æ•°æ®**

![](images/1.png)

**Train Modelè®­ç»ƒæ¨¡å‹**

![](images/2.png)

ç»ƒäº†50ä¸ªepoch 

æ¯”è¾ƒæ··ä¹±

![](images/3.png)

ç»ƒåˆ°okçš„çŠ¶æ€å³å¯

# ğŸ“ŒModel Designæ¨¡å‹è®¾è®¡

æœ¬é¡¹ç›®åŸºäº Transformer æ¶æ„å®ç°äº†ä¸€ä¸ªä¸­æ–‡ GPT-2 æ¨¡å‹ï¼ŒåŒ…å«äº†å¤šä¸ªæ¨¡å—ç»„æˆï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§å’Œçµæ´»æ€§ã€‚ä¸‹é¢æ˜¯æ¨¡å‹è®¾è®¡çš„è¯¦ç»†ä»‹ç»ï¼š

This project implements a Chinese GPT-2 model based on the Transformer architecture, composed of several modules to ensure the model's efficiency and flexibility in natural language processing tasks. Below is a detailed introduction to the model design:

## 1. **è¯åµŒå…¥ä¸ä½ç½®ç¼–ç ï¼ˆEmbedding & Positional Encodingï¼‰**

### è¯åµŒå…¥ï¼ˆEmbeddingï¼‰

æ¨¡å‹ä½¿ç”¨åµŒå…¥å±‚ï¼ˆ`nn.Embedding`ï¼‰å°†è¾“å…¥çš„è¯æ±‡ ID è½¬æ¢ä¸ºä¸€ä¸ªå›ºå®šç»´åº¦çš„åµŒå…¥å‘é‡ã€‚æ¯ä¸ªè¯æ±‡åœ¨æ¨¡å‹ä¸­éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„åµŒå…¥è¡¨ç¤ºï¼Œè¿™äº›åµŒå…¥å‘é‡ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å¾—åˆ°ã€‚

The model uses an embedding layer (`nn.Embedding`) to convert the input word IDs into a fixed-dimensional embedding vector. Each word in the model has a corresponding embedding representation, which is learned during the training process.

### ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

ä¸ºäº†å¤„ç†åºåˆ—ä¸­çš„ä½ç½®å…³ç³»ï¼ŒTransformer æ¨¡å‹ä½¿ç”¨äº†ä½ç½®ç¼–ç ã€‚ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªä¸è¾“å…¥åºåˆ—é•¿åº¦ç›¸åŒçš„å‘é‡ï¼Œå®ƒé€šè¿‡å¯¹ä¸åŒä½ç½®çš„è¯æ±‡åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£åºåˆ—ä¸­è¯æ±‡çš„ç›¸å¯¹ä½ç½®ã€‚

To handle the positional relationships in the sequence, the Transformer model uses positional encoding. Positional encoding is a vector with the same length as the input sequence, created using sine and cosine functions at different positions to help the model understand the relative positions of words in the sequence.
$$
\begin{aligned}
PE_{\text{pos}, 2i} &= \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\
PE_{\text{pos}, 2i+1} &= \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\end{aligned}
$$
è¯¥ç¼–ç æ–¹å¼çš„è®¾è®¡ä½¿å¾—ä¸åŒä½ç½®ä¹‹é—´å…·æœ‰å”¯ä¸€æ€§ï¼Œå¹¶ä¸”ä»»æ„ä¸¤ä¸ªä½ç½®çš„ç›¸å¯¹è·ç¦»æ˜¯å¯ä»¥é€šè¿‡å…¶ç¼–ç å‘é‡çš„çº¿æ€§å˜æ¢è¡¨ç¤ºå‡ºæ¥çš„ã€‚

The design of the positional encoding ensures uniqueness between positions and that the relative distance between any two positions can be represented via linear transformations of their encoding vectors.

### âœ… Key Featuresç‰¹ç‚¹æ€»ç»“ï¼š

- ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ä½¿å¾—ç¼–ç å¯¹ä¸åŒä½ç½®å…·æœ‰å‘¨æœŸæ€§å“åº”ã€‚
- å¯ä»¥æ¨å¹¿åˆ°åºåˆ—é•¿åº¦ä¹‹å¤–ï¼ˆæ”¯æŒæ¨ç†é˜¶æ®µæ›´é•¿åºåˆ—ï¼‰ã€‚
- ç¼–ç å€¼ä¸ä¼šè¢«è®­ç»ƒï¼Œæ˜¯**å›ºå®šçš„ä¸å¯å­¦ä¹ å‚æ•°**ã€‚

- Uses sine and cosine functions to make the encoding exhibit periodic behavior for different positions.
- Can generalize beyond the sequence length (supports inference with longer sequences).
- The encoding values are fixed, non-learnable parameters.

## 2. **å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰**

åœ¨ Transformer æ¶æ„ä¸­ï¼Œæœ€æ ¸å¿ƒçš„ç»„ä»¶æ˜¯å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†æŸä¸€è¯æ±‡æ—¶ï¼Œèƒ½å¤Ÿæ³¨æ„åˆ°è¾“å…¥åºåˆ—ä¸­å…¶ä»–ä½ç½®çš„ç›¸å…³ä¿¡æ¯ï¼Œé¿å…äº†ä¼ ç»Ÿ RNN ä¸­ä¿¡æ¯ä¼ é€’çš„é™åˆ¶ã€‚

The core component in the Transformer architecture is the multi-head self-attention mechanism. This mechanism allows the model to pay attention to relevant information from other positions in the sequence when processing a word, overcoming the limitations of traditional RNNs in information propagation.
$$
\textbf{Step 1: çº¿æ€§æ˜ å°„ï¼ˆLinear Projectionï¼‰}

\begin{aligned}
Q &= XW^Q \\
K &= XW^K \\
V &= XW^V
\end{aligned}

\text{å…¶ä¸­ } X \in \mathbb{R}^{B \times L \times d_{\text{model}}},\quad W^Q, W^K, W^V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}

\\[12pt]

\textbf{Step 2: åˆ†å¤´ï¼ˆSplit into Multiple Headsï¼‰}

\begin{aligned}
Q &\rightarrow Q_{\text{heads}} \in \mathbb{R}^{B \times h \times L \times d_k} \\
K &\rightarrow K_{\text{heads}} \in \mathbb{R}^{B \times h \times L \times d_k} \\
V &\rightarrow V_{\text{heads}} \in \mathbb{R}^{B \times h \times L \times d_k}
\end{aligned}

\text{å…¶ä¸­ } d_k = \frac{d_{\text{model}}}{h},\quad h = \text{å¤´æ•°}
$$

### Mechanism Explanationæœºåˆ¶è¯´æ˜ï¼š

- **å¤šå¤´**ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶è¢«åˆ’åˆ†ä¸ºå¤šä¸ªå¤´ï¼Œæ¯ä¸ªå¤´åˆ†åˆ«ç‹¬ç«‹è®¡ç®—å¹¶æœ€ç»ˆåˆå¹¶ï¼Œå¯ä»¥æ•è·ä¸åŒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- **è‡ªæ³¨æ„åŠ›**ï¼šæ¯ä¸ªè¯æ±‡ä¸è‡ªèº«åŠå…¶ä¸Šä¸‹æ–‡çš„æ‰€æœ‰å…¶ä»–è¯æ±‡è¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ åˆ°è¯æ±‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
- **çº¿æ€§å˜æ¢**ï¼šæ¯ä¸ªå¤´çš„ Qï¼ˆæŸ¥è¯¢ï¼‰ã€Kï¼ˆé”®ï¼‰ã€Vï¼ˆå€¼ï¼‰æ˜¯é€šè¿‡å¯¹è¾“å…¥å‘é‡è¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°çš„ã€‚

- **Multi-head**: The self-attention mechanism is divided into multiple heads, each of which computes independently and is later concatenated, enabling the capture of different contextual information.
- **Self-attention**: Each word interacts with itself and all other words in its context, learning dependencies between words.
- **Linear Transformation**: The Q (query), K (key), and V (value) for each head are obtained by applying linear transformations to the input vector.

## 3. **å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰**

Transformer ä¸­æ¯ä¸€å±‚çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åï¼Œéƒ½ä¼šè·Ÿéšä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰ï¼Œå®ƒç”±ä¸¤ä¸ªå…¨è¿æ¥å±‚ç»„æˆï¼Œé€šå¸¸ä¸­é—´åŠ ä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°ï¼Œç”¨äºå¢å¼ºæ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚

Each layer of the Transformer follows the self-attention mechanism with a feed-forward network. This network consists of two fully connected layers, usually with a ReLU activation function in between, to enhance the model's non-linear expressive power.

- **Structure**: `Linear -> ReLU[Rectified Linear Unit] -> Linear`
- **Function**: The feed-forward network processes each position's representation independently to improve the modelâ€™s representational capability.

- **ç»“æ„**ï¼š`Linear -> ReLU[Rectified Linear Unitï¼ˆçº¿æ€§æ•´æµå•å…ƒï¼‰] -> Linear`
- **ä½œç”¨**ï¼šå‰é¦ˆç¥ç»ç½‘ç»œå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºç‹¬ç«‹è¿›è¡Œå¤„ç†ï¼Œä»è€Œæå‡æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚

$$
\begin{aligned}
\text{FFN}(x) &= \text{Linear}_2(\text{Dropout}(\text{ReLU}(\text{Linear}_1(x)))) \\
&= \max(0, xW_1 + b_1)W_2 + b_2
\end{aligned}
$$

## 4. **å±‚å½’ä¸€åŒ–ä¸æ®‹å·®è¿æ¥ï¼ˆLayer Normalization & Residual Connectionï¼‰**

æ¯ä¸€å±‚çš„è¾“å…¥éƒ½é€šè¿‡æ®‹å·®è¿æ¥ä¸ç»è¿‡å¤„ç†çš„è¾“å‡ºç›¸åŠ ï¼Œå¹¶è¿›è¡Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰ã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºç¼“è§£æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

- **æ®‹å·®è¿æ¥**ï¼šæ¯ä¸€å±‚çš„è¾“å…¥ä¸è¾“å‡ºç›¸åŠ ï¼Œä¿æŒä¿¡æ¯çš„æµåŠ¨ã€‚
- **å±‚å½’ä¸€åŒ–**ï¼šåœ¨æ¯æ¬¡åŠ å’Œä¹‹åè¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚

Each layer's input is added to the processed output through a residual connection and then undergoes layer normalization. This design helps alleviate the vanishing gradient problem in deep neural networks and accelerates the training process.

- **Residual Connection**: The input of each layer is added to its output, preserving information flow.
- **Layer Normalization**: Normalization is applied after the summation to stabilize the training process.

$$
SubLayerOutput = Sublayer(x)\\
LayerOutput = LayerNorm(x + Dropout(SubLayerOutput))\\
Output = LayerNorm(x + Dropout(Sublayer(x)))
$$

## 5. **è¾“å‡ºå±‚ä¸ç”Ÿæˆï¼ˆOutput Layer & Generationï¼‰**

åœ¨ Transformer æ¨¡å‹çš„æœ€åï¼Œé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å±‚ï¼ˆ`nn.Linear`ï¼‰å°†è¾“å‡ºçš„éšå±‚å‘é‡è½¬æ¢ä¸ºè¯æ±‡è¡¨å¤§å°çš„åˆ†å¸ƒï¼Œç”¨äºç”Ÿæˆä¸‹ä¸€è¯çš„é¢„æµ‹ã€‚ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå¯ä»¥é‡‡ç”¨é‡‡æ ·ï¼ˆSamplingï¼‰æˆ–è€…è´ªå©ªè§£ç ï¼ˆGreedy Decodingï¼‰ç­‰ç­–ç•¥ã€‚

At the end of the Transformer model, a linear transformation layer (`nn.Linear`) is used to convert the output hidden layer vector into a distribution of vocabulary size, which is used to predict the next word. During text generation, strategies such as sampling or greedy decoding can be applied.

## 6. **Model Architectureæ¨¡å‹æ¶æ„**

### æ¨¡å‹ç»“æ„å›¾ï¼š

```mermaid
graph TD
    A[è¾“å…¥ Token IDs] --> B[Embedding å±‚]
    B --> C[ä½ç½®ç¼–ç  Positional Encoding]
    C --> D1[Transformer Layer 1]
    D1 --> D2[Transformer Layer 2]
    D2 --> D3[... Transformer Layer N]
    D3 --> E[Linear è¾“å‡ºå±‚]
    E --> F[Softmax ç”Ÿæˆé¢„æµ‹]

    subgraph æ¯ä¸ª Transformer Layer
        D1a[å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ Multi-Head Attention]
        D1b[æ®‹å·®è¿æ¥ + LayerNorm]
        D1c[å‰é¦ˆç¥ç»ç½‘ç»œ Feed-Forward]
        D1d[æ®‹å·®è¿æ¥ + LayerNorm]
        D1 --> D1a --> D1b --> D1c --> D1d --> D2
    end

```

### Detailed Descriptionç»†èŠ‚æè¿°ï¼š

- æ¨¡å‹ç”± **å¤šä¸ª Transformer å±‚** å †å è€Œæˆï¼Œæ¯ä¸ªå±‚åŒ…æ‹¬äº† **è‡ªæ³¨æ„åŠ›å±‚** å’Œ **å‰é¦ˆç¥ç»ç½‘ç»œ**ã€‚
- è¾“å…¥é€šè¿‡åµŒå…¥å±‚å’Œä½ç½®ç¼–ç å±‚è¿›è¡Œå¤„ç†åï¼Œä¼ å…¥å¤šä¸ª Transformer å±‚ï¼Œåœ¨æ¯ä¸€å±‚ä¸­éƒ½ç»è¿‡è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œçš„è®¡ç®—ï¼Œæœ€åç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºã€‚

- The model is composed of **multiple Transformer layers** stacked together, where each layer includes a **self-attention layer** and a **feed-forward network**.
- After the input is processed by the embedding layer and positional encoding layer, it is passed through multiple Transformer layers. In each layer, self-attention and feed-forward network operations are performed, ultimately generating the model's output.

# ğŸ“ŒSteps for Building the Modelæ­å»ºæ­¥éª¤

## 1.Pull the repositoryæ‹‰ä»“åº“

```
!git clone https://gitee.com/jerry-simithgi/chat-model.git
```

## 2.Enter the project directoryè¿›å…¥é¡¹ç›®ç›®å½•

```
cd chat-model/
```

```
cd åŸºäºTransformerçš„å°å‹èŠå¤©æœºå™¨äºº/
```

## 3.Vocabulary Constructionè¯è¡¨æ„å»º

### â…  Generating Formatted Dataç”Ÿæˆæ ¼å¼æ•°æ®

CreateTokernizerAndData\CreateData.py

**åŠ è½½è¾“å…¥æ–‡ä»¶**ï¼š

- ä»£ç ä»åä¸º `qa_final.json` çš„ JSON æ–‡ä»¶ä¸­è¯»å–å¯¹è¯æ•°æ®ã€‚æ¯æ¡æ•°æ®åŒ…å«å¤šä¸ªå›åˆï¼ˆturnsï¼‰ï¼Œæ¯ä¸ªå›åˆæœ‰ä¸€ä¸ª `role`ï¼ˆè§’è‰²ï¼‰å’Œ `text`ï¼ˆæ–‡æœ¬å†…å®¹ï¼‰ã€‚

**æå–é—®ç­”å¯¹**ï¼š

- éå†æ¯æ¡å¯¹è¯ä¸­çš„å›åˆæ•°æ®ï¼Œå¯»æ‰¾ç”¨æˆ· (`role: "user"`) æå‡ºçš„é—®é¢˜å’Œ AI (`role: "ai"`) çš„å›ç­”ã€‚
- æ¯å½“æ‰¾åˆ°ä¸€å¯¹ç”¨æˆ·é—®é¢˜å’Œ AI å›ç­”ï¼Œä»£ç ä¼šå°†å…¶æ ¼å¼åŒ–ä¸º `BOS + prompt + SEP + response + EOS`ï¼Œå…¶ä¸­ `BOS` æ˜¯å¼€å§‹æ ‡è®°ï¼Œ`SEP` æ˜¯åˆ†éš”ç¬¦ï¼Œ`EOS` æ˜¯ç»“æŸæ ‡è®°ã€‚è¿™ç§æ ¼å¼æ˜¯ GPT-2 æ¨¡å‹æ‰€éœ€è¦çš„ã€‚

**ä¿å­˜å¤„ç†åçš„æ•°æ®**ï¼š

- æ ¼å¼åŒ–åçš„æ¯ä¸€å¯¹é—®ç­”å¯¹ä¼šè¢«å†™å…¥åˆ° `train.txt` æ–‡ä»¶ä¸­ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªé—®ç­”å¯¹ã€‚

**è¾“å‡ºæç¤º**ï¼š

- åœ¨å®Œæˆæå–å’Œä¿å­˜æ“ä½œåï¼Œä»£ç ä¼šæ‰“å°å‡ºå·²æå–çš„é—®ç­”å¯¹æ•°é‡ï¼Œå¹¶æç¤ºç”¨æˆ·æ•°æ®å·²ä¿å­˜è‡³ `train.txt` æ–‡ä»¶ä¸­ã€‚

è¿è¡Œè¯¥æ–‡ä»¶å¯ä»¥ç”Ÿæˆæ ¼å¼æ•°æ®

**Loading the Input File**:

- The code reads conversation data from a JSON file named `qa_final.json`. Each entry contains multiple turns, with each turn having a `role` (e.g., "user" or "ai") and `text` (the content of the dialogue).

**Extracting Q&A Pairs**:

- The code loops through each turn in the conversation to find user (`role: "user"`) questions and AI (`role: "ai"`) answers.
- Each time a user question and AI answer are found, the code formats them as `BOS + prompt + SEP + response + EOS`, where `BOS` is the beginning token, `SEP` is the separator token, and `EOS` is the end token. This format is required by the GPT-2 model.

**Saving the Processed Data**:

- The formatted Q&A pairs are written into a `train.txt` file, with each line representing one Q&A pair.

**Output Message**:

- After the extraction and saving operations are completed, the code prints the number of extracted Q&A pairs and informs the user that the data has been saved to the `train.txt` file.

Running this file will generate the formatted data.

```
qa_final.jsonï¼š
[
    {
        "turns": [
            {
                "role": "user",
                "text": "ä½ å¥½ï¼Œæˆ‘ç°åœ¨å‡†å¤‡åšä¸€åå°çº¢ä¹¦ç¾å¦†åšä¸»ï¼Œè¯·é—®æˆ‘è¯¥åšäº›ä»€ä¹ˆå—ï¼Ÿå¯ä»¥ç»™æˆ‘ä¸€äº›è¯¦ç»†çš„å‡†å¤‡list"
            },
            {
                "role": "ai",
                "text": "å½“ç„¶! ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥é‡‡å–çš„ä¸€äº›æ­¥éª¤æ¥ä¸ºæ‚¨çš„å°çº¢ä¹¦ç¾å¦†åšä¸»ä¹‹æ—…åšå‡†å¤‡ï¼š\n1.é€‰æ‹©ä¸€ä¸ªåˆ©åŸº:ç¡®å®šä½ æƒ³è¦å…³æ³¨çš„ç‰¹å®šç¾å®¹é¢†åŸŸã€‚æ‚¨å¯¹æŠ¤è‚¤ã€å½©å¦†ã€æŠ¤å‘æˆ–è¿™äº›çš„ç»„åˆæ„Ÿå…´è¶£å—? ç¼©å°ä½ çš„åˆ©åŸºå°†å¸®åŠ©ä½ åˆ›å»ºæ›´æœ‰é’ˆå¯¹æ€§çš„å†…å®¹ï¼Œä½ çš„è§‚ä¼—ä¼šå–œæ¬¢ã€‚\n2.ç ”ç©¶æ‚¨çš„å—ä¼—: é€šè¿‡æµè§ˆçƒ­é—¨çš„å°çº¢ä¹¦ç¾å¦†è´¦å·äº†è§£æ‚¨çš„æ½œåœ¨å—ä¼—ã€‚è§‚å¯Ÿä»–ä»¬å‚ä¸æœ€å¤šçš„å†…å®¹ä»¥åŠä»–ä»¬æœ€æ„Ÿå…´è¶£çš„ä¸»é¢˜ã€‚\n3.åˆ¶å®šå†…å®¹è®¡åˆ’:é’ˆå¯¹æ‚¨å°†åˆ›å»ºçš„å†…å®¹ç±»å‹å’Œé¢‘ç‡åˆ¶å®šè®¡åˆ’ã€‚è€ƒè™‘å“ªç§æ ¼å¼æœ€é€‚åˆæ‚¨çš„åˆ©åŸºå¸‚åœºï¼Œä¾‹å¦‚è§†é¢‘ã€ç…§ç‰‡æˆ–ä¹¦é¢è¯„è®ºã€‚æå‰è®¡åˆ’æ‚¨çš„å¸–å­ä»¥ä¿æŒä¸€è‡´çš„æ—¶é—´è¡¨ã€‚\n4.å»ºç«‹æ‚¨çš„ä¸ªäººèµ„æ–™:åˆ›å»ºä»£è¡¨æ‚¨å“ç‰Œçš„ä¸“ä¸šä¸”å…·æœ‰è§†è§‰å¸å¼•åŠ›çš„ä¸ªäººèµ„æ–™ã€‚ä½¿ç”¨é«˜è´¨é‡çš„å›¾åƒï¼Œå¹¶åŒ…å«å‡†ç¡®åæ˜ æ‚¨çš„åˆ©åŸºå’Œä¸ªæ€§çš„ç®€å†ã€‚\n5.å‘å±•åˆä½œä¼™ä¼´å…³ç³»: åœ¨å°çº¢ä¹¦ä¸Šä¸å…¶ä»–ç¾å¦†åšä¸»ã€å“ç‰Œå’Œå½±å“è€…å»ºç«‹è”ç³»ã€‚ä¸ä¸šå†…å…¶ä»–äººå»ºç«‹å…³ç³»æœ‰åŠ©äºæé«˜çŸ¥ååº¦å’Œåˆä½œæœºä¼š\n6.ä¿æŒæœ€æ–°:äº†è§£ç¾å®¹è¡Œä¸šçš„æœ€æ–°è¶‹åŠ¿å’Œæ–°é—»ã€‚å‚åŠ è¡Œä¸šæ´»åŠ¨ï¼Œé˜…è¯»ç›¸å…³å‡ºç‰ˆç‰©ï¼Œéšæ—¶äº†è§£æœ€æ–°çš„äº§å“å’ŒæŠ€æœ¯ã€‚\n7ï¼Œä¸æ‚¨çš„è§‚ä¼—äº’åŠ¨:é€šè¿‡å›å¤è¯„è®ºå’Œæ¶ˆæ¯ä»¥åŠé¼“åŠ±å¯¹æ‚¨çš„å¸–å­è¿›è¡Œè®¨è®ºæ¥ä¸æ‚¨çš„å…³æ³¨è€…äº’åŠ¨ã€‚è¿™å°†æœ‰åŠ©äºå»ºç«‹å¿ å®çš„è¿½éšè€…å¹¶å»ºç«‹æ‚¨åœ¨ç¾å®¹è¡Œä¸šçš„æƒå¨ã€‚\nè¯·è®°ä½åœ¨æ‚¨çš„å†…å®¹åˆ›ä½œä¸­ä¿æŒçœŸå®ã€çƒ­æƒ…å’Œä¸€è‡´ã€‚ç¥ä½ çš„å°çº¢ä¹¦ç¾å¦†åšä¸»ä¹‹æ—…é¡ºåˆ©!"
            }
        ]
    },...
ç»è¿‡è¯¥æ–‡ä»¶å¤„ç†åå¾—åˆ°
The result obtained after processing this file.
<bos>ä½ å¥½ï¼Œæˆ‘ç°åœ¨å‡†å¤‡åšä¸€åå°çº¢ä¹¦ç¾å¦†åšä¸»ï¼Œè¯·é—®æˆ‘è¯¥åšäº›ä»€ä¹ˆå—ï¼Ÿå¯ä»¥ç»™æˆ‘ä¸€äº›è¯¦ç»†çš„å‡†å¤‡list<sep>å½“ç„¶! ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥é‡‡å–çš„ä¸€äº›æ­¥éª¤æ¥ä¸ºæ‚¨çš„å°çº¢ä¹¦ç¾å¦†åšä¸»ä¹‹æ—…åšå‡†å¤‡ï¼š 1.é€‰æ‹©ä¸€ä¸ªåˆ©åŸº:ç¡®å®šä½ æƒ³è¦å…³æ³¨çš„ç‰¹å®šç¾å®¹é¢†åŸŸã€‚æ‚¨å¯¹æŠ¤è‚¤ã€å½©å¦†ã€æŠ¤å‘æˆ–è¿™äº›çš„ç»„åˆæ„Ÿå…´è¶£å—? ç¼©å°ä½ çš„åˆ©åŸºå°†å¸®åŠ©ä½ åˆ›å»ºæ›´æœ‰é’ˆå¯¹æ€§çš„å†…å®¹ï¼Œä½ çš„è§‚ä¼—ä¼šå–œæ¬¢ã€‚ 2.ç ”ç©¶æ‚¨çš„å—ä¼—: é€šè¿‡æµè§ˆçƒ­é—¨çš„å°çº¢ä¹¦ç¾å¦†è´¦å·äº†è§£æ‚¨çš„æ½œåœ¨å—ä¼—ã€‚è§‚å¯Ÿä»–ä»¬å‚ä¸æœ€å¤šçš„å†…å®¹ä»¥åŠä»–ä»¬æœ€æ„Ÿå…´è¶£çš„ä¸»é¢˜ã€‚ 3.åˆ¶å®šå†…å®¹è®¡åˆ’:é’ˆå¯¹æ‚¨å°†åˆ›å»ºçš„å†…å®¹ç±»å‹å’Œé¢‘ç‡åˆ¶å®šè®¡åˆ’ã€‚è€ƒè™‘å“ªç§æ ¼å¼æœ€é€‚åˆæ‚¨çš„åˆ©åŸºå¸‚åœºï¼Œä¾‹å¦‚è§†é¢‘ã€ç…§ç‰‡æˆ–ä¹¦é¢è¯„è®ºã€‚æå‰è®¡åˆ’æ‚¨çš„å¸–å­ä»¥ä¿æŒä¸€è‡´çš„æ—¶é—´è¡¨ã€‚ 4.å»ºç«‹æ‚¨çš„ä¸ªäººèµ„æ–™:åˆ›å»ºä»£è¡¨æ‚¨å“ç‰Œçš„ä¸“ä¸šä¸”å…·æœ‰è§†è§‰å¸å¼•åŠ›çš„ä¸ªäººèµ„æ–™ã€‚ä½¿ç”¨é«˜è´¨é‡çš„å›¾åƒï¼Œå¹¶åŒ…å«å‡†ç¡®åæ˜ æ‚¨çš„åˆ©åŸºå’Œä¸ªæ€§çš„ç®€å†ã€‚ 5.å‘å±•åˆä½œä¼™ä¼´å…³ç³»: åœ¨å°çº¢ä¹¦ä¸Šä¸å…¶ä»–ç¾å¦†åšä¸»ã€å“ç‰Œå’Œå½±å“è€…å»ºç«‹è”ç³»ã€‚ä¸ä¸šå†…å…¶ä»–äººå»ºç«‹å…³ç³»æœ‰åŠ©äºæé«˜çŸ¥ååº¦å’Œåˆä½œæœºä¼š 6.ä¿æŒæœ€æ–°:äº†è§£ç¾å®¹è¡Œä¸šçš„æœ€æ–°è¶‹åŠ¿å’Œæ–°é—»ã€‚å‚åŠ è¡Œä¸šæ´»åŠ¨ï¼Œé˜…è¯»ç›¸å…³å‡ºç‰ˆç‰©ï¼Œéšæ—¶äº†è§£æœ€æ–°çš„äº§å“å’ŒæŠ€æœ¯ã€‚ 7ï¼Œä¸æ‚¨çš„è§‚ä¼—äº’åŠ¨:é€šè¿‡å›å¤è¯„è®ºå’Œæ¶ˆæ¯ä»¥åŠé¼“åŠ±å¯¹æ‚¨çš„å¸–å­è¿›è¡Œè®¨è®ºæ¥ä¸æ‚¨çš„å…³æ³¨è€…äº’åŠ¨ã€‚è¿™å°†æœ‰åŠ©äºå»ºç«‹å¿ å®çš„è¿½éšè€…å¹¶å»ºç«‹æ‚¨åœ¨ç¾å®¹è¡Œä¸šçš„æƒå¨ã€‚ è¯·è®°ä½åœ¨æ‚¨çš„å†…å®¹åˆ›ä½œä¸­ä¿æŒçœŸå®ã€çƒ­æƒ…å’Œä¸€è‡´ã€‚ç¥ä½ çš„å°çº¢ä¹¦ç¾å¦†åšä¸»ä¹‹æ—…é¡ºåˆ©!<eos>
```

### â…¡ Generate Vocabulary Fileç”Ÿæˆè¯è¡¨æ–‡ä»¶

CreateTokernizerAndData\tokenizer_custom.py

**æ„å»ºè¯è¡¨** (`build_vocab_from_file`)ï¼š

- ä»è¾“å…¥çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆ`train.txt`ï¼‰ä¸­æå–æ‰€æœ‰å­—ç¬¦ï¼Œç»Ÿè®¡æ¯ä¸ªå­—ç¬¦çš„å‡ºç°é¢‘ç‡ã€‚
- åŸºäºæœ€å°é¢‘ç‡ï¼ˆ`min_freq`ï¼‰ç­›é€‰å‡ºå‡ºç°é¢‘ç‡å¤§äºç­‰äºè¯¥é˜ˆå€¼çš„å­—ç¬¦ï¼Œæ„å»ºè¯è¡¨ã€‚
- åœ¨è¯è¡¨ä¸­åŠ å…¥ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼š`<pad>`ï¼ˆå¡«å……ç¬¦ï¼‰ã€`<unk>`ï¼ˆæœªçŸ¥ç¬¦ï¼‰ã€`<bos>`ï¼ˆå¼€å§‹æ ‡è®°ï¼‰ã€`<sep>`ï¼ˆåˆ†éš”ç¬¦ï¼‰ã€`<eos>`ï¼ˆç»“æŸæ ‡è®°ï¼‰ã€‚
- ç”Ÿæˆä¸€ä¸ªå­—ç¬¦åˆ° ID çš„æ˜ å°„ï¼ˆ`token2id`ï¼‰å’Œ ID åˆ°å­—ç¬¦çš„æ˜ å°„ï¼ˆ`id2token`ï¼‰ã€‚

**ä¿å­˜è¯è¡¨åˆ°æ–‡ä»¶** (`save_vocab_to_file`)ï¼š

- å°†æ„å»ºå¥½çš„è¯è¡¨ï¼ˆ`token2id` å’Œ `id2token`ï¼‰ä¿å­˜ä¸º JSON æ ¼å¼ï¼Œå­˜å‚¨åœ¨æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„ï¼ˆ`vocab.json`ï¼‰ä¸­ã€‚

**ä»æ–‡ä»¶åŠ è½½è¯è¡¨** (`load_vocab_from_file`)ï¼š

- ä»ä¿å­˜çš„è¯è¡¨æ–‡ä»¶ä¸­åŠ è½½è¯è¡¨ï¼ˆ`token2id` å’Œ `id2token`ï¼‰ï¼Œå¹¶å°† ID æ˜ å°„çš„é”®ä»å­—ç¬¦ä¸²ç±»å‹è½¬å›æ•´æ•°ç±»å‹ã€‚

**æ–‡æœ¬ç¼–ç ** (`encode`)ï¼š

- å°†è¾“å…¥çš„æ–‡æœ¬ï¼ˆå¦‚ä¸€å¥è¯ï¼‰è½¬æ¢ä¸ºä¸€ç³»åˆ—çš„ Token IDï¼Œä½¿ç”¨è¯è¡¨ä¸­çš„æ˜ å°„ã€‚
- åœ¨æ–‡æœ¬çš„å¼€å¤´å’Œç»“å°¾åˆ†åˆ«æ·»åŠ  `<bos>` å’Œ `<eos>` æ ‡è®°ã€‚
- å¦‚æœæ–‡æœ¬é•¿åº¦è¶…è¿‡æœ€å¤§é•¿åº¦ `max_len`ï¼Œåˆ™æˆªæ–­ï¼›å¦‚æœæ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œåˆ™ç”¨ `<pad>` å¡«å……ã€‚

**æ–‡æœ¬è§£ç ** (`decode`)ï¼š

- å°†ç¼–ç åçš„ Token ID è½¬æ¢å›åŸå§‹æ–‡æœ¬ï¼ˆå­—ç¬¦ï¼‰ã€‚
- åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œå¦‚æœé‡åˆ° `<eos>`ï¼Œåˆ™åœæ­¢è§£ç ã€‚
- è·³è¿‡ç‰¹æ®Šç¬¦å· `<pad>`, `<unk>`, `<bos>`ã€‚

åŸºäºtrain.txtç”Ÿæˆvocab.jsonï¼ˆè¯è¡¨ï¼‰

**Build Vocabulary** (`build_vocab_from_file`):

- Extract all characters from the input text file (`train.txt`) and count the frequency of each character.
- Filter out characters with a frequency greater than or equal to a given minimum frequency (`min_freq`) to build the vocabulary.
- Add special tokens to the vocabulary: `<pad>` (padding), `<unk>` (unknown), `<bos>` (beginning of sentence), `<sep>` (separator), and `<eos>` (end of sentence).
- Create two mappings: a character-to-ID mapping (`token2id`) and an ID-to-character mapping (`id2token`).

**Save Vocabulary to File** (`save_vocab_to_file`):

- Save the constructed vocabulary (`token2id` and `id2token`) as a JSON file to the specified path (`vocab.json`).

**Load Vocabulary from File** (`load_vocab_from_file`):

- Load the vocabulary (`token2id` and `id2token`) from the saved file and convert the keys of the ID mapping from strings back to integers.

**Text Encoding** (`encode`):

- Convert an input text (such as a sentence) into a series of Token IDs using the vocabulary mappings.
- Add `<bos>` at the beginning and `<eos>` at the end of the text.
- If the text exceeds the maximum length (`max_len`), truncate it; if it's too short, pad it with `<pad>`.

**Text Decoding** (`decode`):

- Convert the encoded Token IDs back into the original text (characters).
- Stop decoding when the `<eos>` token is encountered.
- Skip over special tokens like `<pad>`, `<unk>`, and `<bos>`.

Generate `vocab.json` (Vocabulary) based on `train.txt`.

## 4.Generate Dataset for Trainingç”Ÿæˆç”¨äºè®­ç»ƒçš„æ•°æ®é›†

CreateTokernizerAndData\prepare_datasetQA2.py

**é…ç½®é¡¹å’ŒåŠ è½½è¯è¡¨**ï¼š

- è®¾ç½®äº†æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼ˆ`max_len`ï¼‰å’Œè¾“å…¥ã€è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
- åŠ è½½äº‹å…ˆæ„å»ºå¥½çš„è¯è¡¨ï¼ˆ`vocab.json`ï¼‰å’Œæ˜ å°„ï¼ŒåŒ…å«ä»å­—ç¬¦åˆ° ID çš„æ˜ å°„ï¼ˆ`token2id`ï¼‰å’Œ ID åˆ°å­—ç¬¦çš„æ˜ å°„ï¼ˆ`id2token`ï¼‰ã€‚
- å¦‚æœè¯è¡¨ä¸­ç¼ºå°‘ç‰¹æ®Šæ ‡è®°ï¼ˆ`<bos>`, `<sep>`, `<eos>`, `<pad>`ï¼‰ï¼Œä¼šè‡ªåŠ¨æ·»åŠ å¹¶æ›´æ–°è¯è¡¨ã€‚

**å¤„ç†å•è¡Œæ•°æ®**ï¼š

- `process_line` å‡½æ•°ç”¨äºå¤„ç†æ¯ä¸€è¡ŒåŸå§‹æ–‡æœ¬æ•°æ®ï¼š
  - æ ¡éªŒæ•°æ®æ ¼å¼ï¼Œç¡®ä¿æ¯è¡Œæ•°æ®åŒ…å« `<bos>`, `<sep>`, `<eos>`ã€‚
  - å°†é—®é¢˜ï¼ˆquestionï¼‰å’Œå›ç­”ï¼ˆanswerï¼‰éƒ¨åˆ†æå–å‡ºæ¥ï¼Œå¹¶æ·»åŠ é€‚å½“çš„ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ `<bos>` å’Œ `<eos>`ï¼‰ã€‚
  - ä½¿ç”¨ `encode` å‡½æ•°å°†é—®é¢˜å’Œå›ç­”è½¬æ¢ä¸º Token ID åºåˆ—ã€‚
  - å¡«å……æˆ–æˆªæ–­ Token ID åºåˆ—ï¼Œç¡®ä¿æ¯ä¸ªåºåˆ—çš„é•¿åº¦ä¸è¶…è¿‡æœ€å¤§é•¿åº¦ `max_len`ã€‚
  - å¦‚æœé—®é¢˜æˆ–å›ç­”ä¸­ä¸åŒ…å« `<eos>` æ ‡è®°ï¼Œåˆ™ä¸¢å¼ƒè¯¥è¡Œæ•°æ®ã€‚

**ä¿å­˜å¤„ç†ç»“æœ**ï¼š

- å¯¹æ¯ä¸€è¡Œæ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¤„ç†æˆåŠŸåå°†å…¶ä¿å­˜ä¸º JSON æ ¼å¼ï¼ˆ`input_ids` å’Œ `labels`ï¼‰ã€‚
- ç»“æœä»¥ JSON è¡Œï¼ˆ`.jsonl`ï¼‰æ ¼å¼ä¿å­˜ï¼Œä¾¿äºåç»­çš„è®­ç»ƒä½¿ç”¨ã€‚

**æ‰§è¡Œå’Œè¾“å‡º**ï¼š

- è¯»å–åŸå§‹è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼ˆ`train.txt`ï¼‰ï¼Œé€è¡Œå¤„ç†å¹¶å°†ç»“æœå†™å…¥è¾“å‡ºæ–‡ä»¶ï¼ˆ`train_encoded_v2.jsonl`ï¼‰ã€‚
- è¾“å‡ºå¤„ç†æˆåŠŸçš„è®­ç»ƒæ ·æœ¬æ•°ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªå¤„ç†åçš„æ•°æ®æ–‡ä»¶ã€‚

åŸºäºvocab.jsonå’Œtrain.txtç”Ÿæˆç”¨äºè®­ç»ƒçš„æ•°æ®é›†

**Configuration and Vocabulary Loading**:

- Set the maximum text length (`max_len`) and input/output file paths.
- Load the pre-built vocabulary (`vocab.json`) and mappings, which include the character-to-ID mapping (`token2id`) and ID-to-character mapping (`id2token`).
- If any special tokens (`<bos>`, `<sep>`, `<eos>`, `<pad>`) are missing from the vocabulary, they will be automatically added and the vocabulary will be updated.

**Process Each Line of Data**:

- The `process_line` function processes each line of the raw text data:
  - Verify the data format, ensuring each line contains `<bos>`, `<sep>`, and `<eos>`.
  - Extract the question and answer parts, and add the appropriate special tokens (such as `<bos>` and `<eos>`).
  - Use the `encode` function to convert the question and answer into Token ID sequences.
  - Pad or truncate the Token ID sequences to ensure that each sequence does not exceed the maximum length (`max_len`).
  - If a question or answer does not contain the `<eos>` token, the line is discarded.

**Save Processed Results**:

- For each processed line, after successful processing, save it as a JSON format with `input_ids` and `labels`.
- The results are saved in a JSON Lines (`.jsonl`) format, which is convenient for later training use.

**Execution and Output**:

- Read the raw training data file (`train.txt`), process it line by line, and write the results to the output file (`train_encoded_v2.jsonl`).
- Output the number of successfully processed training samples and generate a processed data file.

Generate the training dataset based on `vocab.json` and `train.txt`.

```
<bos>â€œå®«å»·ç‰æ¶²é…’â€ä¸‹ä¸€å¥æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå¤šå°‘é’±ä¸€æ¯ï¼Ÿ<sep>â€œå®«å»·ç‰æ¶²é…’â€çš„ä¸‹ä¸€å¥æ˜¯â€œä¸€ç™¾å…«ä¸€æ¯â€ã€‚å®ƒçš„ä»·æ ¼æ˜¯ä¸€ç™¾å…«åå…ƒä¸€æ¯ã€‚<eos>
train.txt

ç»è¿‡å¤„ç†å
å¾—åˆ°
{"input_ids": [2, 5, 6, 7, 8, 9, 28, 97, 261, 515, 568, 131, 569, 570, 571, 572, 12, 573, 574, 575, 576, 409, 19, 577, 270, 578, 203, 357, 19, 579, 446, 580, 81, 5, 45, 7, 8, 9, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "labels": [2, 5, 6, 7, 8, 9, 581, 582, 51, 569, 583, 41, 584, 570, 142, 585, 568, 41, 24, 586, 12, 19, 148, 515, 587, 142, 588, 589, 41, 92, 184, 590, 159, 290, 58, 81, 409, 591, 85, 574, 159, 142, 279, 48, 586, 577, 592, 59, 19, 542, 41, 593, 107, 586, 594, 5, 45, 7, 8, 9, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
2æ˜¯<bos> 4æ˜¯<eos>
åœ¨æ¯ä¸ªq(â€œå®«å»·ç‰æ¶²é…’â€ä¸‹ä¸€å¥æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå¤šå°‘é’±ä¸€æ¯ï¼Ÿ),a(â€œå®«å»·ç‰æ¶²é…’â€çš„ä¸‹ä¸€å¥æ˜¯â€œä¸€ç™¾å…«ä¸€æ¯â€ã€‚å®ƒçš„ä»·æ ¼æ˜¯ä¸€ç™¾å…«åå…ƒä¸€æ¯ã€‚)å‰åéƒ½åŠ ä¸Š<bos> å’Œ <eos>åè¿›è¡Œç¼–ç ã€‚
```

## 5.Trainingè®­ç»ƒ

train.py

**é…ç½®å‚æ•°**ï¼š

- è®¾ç½®äº†è®­ç»ƒçš„è¶…å‚æ•°ï¼ŒåŒ…æ‹¬è¯è¡¨æ–‡ä»¶ã€è®­ç»ƒæ•°æ®æ–‡ä»¶ã€æ‰¹å¤„ç†å¤§å°ã€æœ€å¤§åºåˆ—é•¿åº¦ã€è®­ç»ƒè½®æ•°ã€å­¦ä¹ ç‡ç­‰ã€‚
- æŒ‡å®šäº†æ¨¡å‹ä¿å­˜ç›®å½•å’Œæ—¥å¿—æ–‡ä»¶ï¼Œç”¨äºä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ªé˜¶æ®µçš„æ¨¡å‹åŠè®­ç»ƒæ—¥å¿—ã€‚

**è¯»å–è¯è¡¨**ï¼š

- é€šè¿‡ `load_vocab_from_file` å‡½æ•°åŠ è½½è¯è¡¨æ–‡ä»¶ï¼ˆ`vocab.json`ï¼‰ï¼Œè·å–è¯åˆ° ID çš„æ˜ å°„ï¼ˆ`token2id`ï¼‰å’Œ ID åˆ°è¯çš„æ˜ å°„ï¼ˆ`id2token`ï¼‰ã€‚
- è¾“å‡ºè¯è¡¨çš„å¤§å°ï¼Œä¾›åç»­æ¨¡å‹æ„å»ºæ—¶ä½¿ç”¨ã€‚

**è‡ªå®šä¹‰æ•°æ®é›†**ï¼š

- `QADataset` ç±»è´Ÿè´£è¯»å–è®­ç»ƒæ•°æ®ï¼ˆå­˜å‚¨åœ¨ `train_encoded_v2.jsonl` æ–‡ä»¶ä¸­ï¼‰ï¼Œå¯¹æ¯ä¸€è¡Œæ•°æ®è¿›è¡Œå¤„ç†ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹çš„è¾“å…¥æ ¼å¼ã€‚
- æ¯ä¸ªæ ·æœ¬åŒ…å« `input_ids` å’Œ `labels`ï¼Œå¹¶å¡«å……åˆ°æœ€å¤§é•¿åº¦ `max_len`ã€‚
- æ•°æ®é›†ä½¿ç”¨ PyTorch çš„ `Dataset` ç±»å°è£…ï¼Œå¹¶é€šè¿‡ `DataLoader` æä¾›æ‰¹å¤„ç†æ•°æ®ã€‚

**æ„å»ºæ¨¡å‹**ï¼š

- ä½¿ç”¨è‡ªå®šä¹‰çš„ `GPT2Transformer` ç±»æ„å»º GPT-2 æ¨¡å‹ï¼Œæ¨¡å‹åŒ…æ‹¬è¯è¡¨å¤§å°ã€åµŒå…¥ç»´åº¦ã€æ³¨æ„åŠ›å¤´æ•°ã€å±‚æ•°ç­‰è¶…å‚æ•°ã€‚
- è®¡ç®—æ¨¡å‹çš„æ€»å‚æ•°é‡ï¼Œå¹¶è¾“å‡ºã€‚

**æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨**ï¼š

- ä½¿ç”¨ `CrossEntropyLoss` ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¿½ç•¥å¡«å…… token (`<pad>`)ã€‚
- ä¼˜åŒ–å™¨ä½¿ç”¨ Adamï¼Œå­¦ä¹ ç‡ä¸º `1e-4`ï¼Œå¹¶ä½¿ç”¨ `StepLR` å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œåœ¨æ¯ 50 è½®åå°†å­¦ä¹ ç‡å‡åŠã€‚

**åŠ è½½é¢„è®­ç»ƒæ¨¡å‹**ï¼š

- æä¾›äº† `load_model` å‡½æ•°ç”¨äºåŠ è½½å·²ä¿å­˜çš„æ¨¡å‹å‚æ•°ï¼Œå¦‚æœæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ï¼Œåˆ™ç»§ç»­è®­ç»ƒï¼›å¦‚æœæ‰¾ä¸åˆ°ï¼Œåˆ™ä»å¤´å¼€å§‹è®­ç»ƒã€‚

**è®­ç»ƒå‡½æ•°**ï¼š

- `train` å‡½æ•°æ‰§è¡Œæ¨¡å‹çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š
  - åŠ è½½æ•°æ®å¹¶è¿›è¡Œè®­ç»ƒã€‚
  - æ¯ä¸ª epoch ä¸­è®¡ç®—æŸå¤±ï¼Œè¿›è¡Œåå‘ä¼ æ’­å’Œä¼˜åŒ–æ­¥éª¤ã€‚
  - æ¯è®­ç»ƒä¸€å®šè½®æ•°ï¼ˆå¦‚æ¯ 20 è½®ï¼‰ä¿å­˜ä¸€æ¬¡æ¨¡å‹å‚æ•°ã€‚
  - è®°å½•æ¯ä¸ª epoch çš„æŸå¤±å’Œå­¦ä¹ ç‡ï¼Œå¹¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ä¸­ã€‚

**æœ€ç»ˆä¿å­˜æ¨¡å‹**ï¼š

- è®­ç»ƒç»“æŸåï¼Œæœ€ç»ˆä¿å­˜æ¨¡å‹å‚æ•°åˆ°æŒ‡å®šè·¯å¾„ï¼ˆ`final_model.pth`ï¼‰ã€‚

**å›¾å½¢ä¸æ—¥å¿—è®°å½•**ï¼š

- æ–‡ä»¶ä¸­è¿˜åŒ…å«äº†æ—¥å¿—æ–‡ä»¶è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå­¦ä¹ ç‡ï¼Œå¹¶å¯é€šè¿‡å›¾å½¢åŒ–å·¥å…·ï¼ˆå¦‚ matplotlibï¼‰è¿›è¡Œè®­ç»ƒç»“æœçš„å¯è§†åŒ–ï¼ˆåœ¨è¯¥æ–‡ä»¶å†…å¹¶æœªä½¿ç”¨ï¼Œä½†é€šå¸¸å¯ç”¨äºç»˜åˆ¶æŸå¤±æ›²çº¿ï¼‰ã€‚

è®­ç»ƒåå¯ä»¥å¾—åˆ°è®­ç»ƒçš„å‚æ•°æ–‡ä»¶

å¦‚æœä½ æƒ³é‡å¤´è®­ç»ƒå¯ä»¥ç§»èµ°æˆ–åˆ é™¤æ–‡ä»¶ï¼ˆsaved_models\final_model.pthï¼‰

**Configuration Parameters**:

- Sets training hyperparameters, including vocabulary file, training data file, batch size, maximum sequence length, number of epochs, learning rate, and more.
- Specifies the model save directory and log file to store model checkpoints and training logs during the training process.

**Read Vocabulary**:

- The `load_vocab_from_file` function is used to load the vocabulary file (`vocab.json`), retrieving the token-to-ID mapping (`token2id`) and ID-to-token mapping (`id2token`).
- Outputs the size of the vocabulary, which will be used for building the model later.

**Custom Dataset**:

- The `QADataset` class handles reading the training data (stored in `train_encoded_v2.jsonl` file), processing each line of data, and converting the text into a format suitable for the model.
- Each sample contains `input_ids` and `labels`, and these are padded to the maximum length (`max_len`).
- The dataset is wrapped in PyTorch's `Dataset` class and batched using `DataLoader`.

**Model Construction**:

- The custom `GPT2Transformer` class is used to build the GPT-2 model, including hyperparameters such as vocabulary size, embedding dimension, number of attention heads, and the number of layers.
- The total number of model parameters is calculated and outputted.

**Loss Function and Optimizer**:

- `CrossEntropyLoss` is used as the loss function, with the padding token (`<pad>`) ignored.
- The optimizer is Adam with a learning rate of `1e-4`, and the learning rate scheduler (`StepLR`) halves the learning rate every 50 epochs.

**Load Pretrained Model**:

- The `load_model` function is provided to load the saved model parameters. If a model file is found, training continues from the last saved checkpoint. If not, training starts from scratch.

**Training Function**:

- The `train` function executes the model training workflow, including:
  - Loading data and performing training.
  - Calculating loss during each epoch, performing backpropagation, and updating the model.
  - Saving the model parameters every certain number of epochs (e.g., every 20 epochs).
  - Recording the loss and learning rate for each epoch and saving them in the log file.

**Final Model Save**:

- After training finishes, the model parameters are saved to the specified path (`final_model.pth`).

**Graphical and Log Recording**:

- The file includes logging functionality to record the loss and learning rate throughout the training process. These logs can be visualized using graphical tools like `matplotlib` (although not used in this specific file, it is typically used to plot loss curves).

After training, you will obtain the trained parameter file.

If you want to train from scratch, you can remove or delete the file (`saved_models/final_model.pth`).

## 6.Test the model's performance.æµ‹è¯•æ¨¡å‹æ•ˆæœ

solveProblem.py

```
model_path = 'saved_models/epoch_090.pth'
#å°†æ–‡ä»¶å†…æ¨¡å‹çš„å‚æ•°é€‰æ‹©ä½ è®­ç»ƒå¥½çš„å‚æ•°ï¼Œå¯åŠ¨è¯¥è„šæœ¬å³å¯äº¤äº’è§‚å¯Ÿè®­ç»ƒæ•ˆæœ
#Select the parameters of the model in the file to be the parameters you trained, then run the script to interactively observe the training results.
```

